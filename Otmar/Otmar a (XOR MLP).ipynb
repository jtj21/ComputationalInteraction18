{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjnXexNzY6zs"
   },
   "source": [
    "<img src=\"data/banner.jpg\" width=\"100%\">\n",
    "\n",
    "---------------\n",
    "Day 4 of the [*ACM SIGCHI Summer School on Computational Interaction*](https://www-edc.eng.cam.ac.uk/summerschool). August 2018, in Cambridge, UK.\n",
    "\n",
    "# Deep Learning - Part I/III\n",
    "In this Jupyter notebook you will implement a simple neural network from scratch.\n",
    "To understand what a NN is we will limit ourselves to a very simple task, namely that of solving the exclusive or (XOR) function. In part II of the course we will then use the acquired concepts (and most of the code) to attack a real HCI problem - that of eye-gaze estimation.\n",
    "\n",
    "\n",
    "Normally, you would use deep-learning libraries such as [TensorFlow](https://tensorflow.org) or [Caffe](http://caffe.berkeleyvision.org/) to build neural network powered applications. However, for learning purposes it is better to actually implement everything yourself in plain Python and NumPy. \n",
    "\n",
    "# Setup\n",
    "First let's import the necessary libraries and let's configure essential building blocks such as a pseudo-random number generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "C_Y7kRBxan7E"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 6\n",
    "\n",
    "RANDOM_SEED = 56\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kgpvohIAbSgG"
   },
   "source": [
    "We have now imported and configured useful helper libraries such as [matlibplot](https://matplotlib.org/) for plotting and [NumPy](http://www.numpy.org/) - the workhorse of numerical and scientific programming. If you are not familiar with these you should follow up on the [various](https://docs.scipy.org/doc/numpy/user/quickstart.html) [tutorials](https://cs231n.github.io/python-numpy-tutorial/) available online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fu1LKyI9cHam"
   },
   "source": [
    "# Activation functions and background\n",
    "\n",
    "During the introduction you have seen that in deep neural networks every linear layer is followed by a non-linear activation function. Without these neural networks would only be able to approximate affine functions and hence would be a lot less powerful. \n",
    "The perceptron algorithm uses a step-function as activation function. However, for various reasons in DL one uses different types of activation functions. One reason is that the activation function should be differentiable and should have a derivative that leads to fast learning rates.\n",
    "\n",
    "# Sigmoid\n",
    "\n",
    "In this tutorial we will be working with the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) as activation function. It has several appealing properties: it is bounded, it is fully differentiable and has a positive derivative at any point. Furthermore, the sigmoid function maps real-valued inputs to the (0,1) range. This is useful because it allows us to interpret its output as a probability value. The sigmoid function is defined by: \n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "In order to train our neural network we will also need it's derivative which is given by:\n",
    "$$\\frac{\\partial \\sigma(x)}{\\partial x}=\\sigma(x)\\cdot(1-\\sigma(x))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DglX2zQTncpE"
   },
   "source": [
    "<div style=\"margin:2em 0.2em;background-color:#eef;border-radius:0.5em;padding:3.5em;padding-bottom:2.7em;color:#444;\">\n",
    "    <div style=\"color:#55a;font-weight:bold;margin:-1.7em 0 1em -1.7em;\">Task 1a</div>\n",
    "    Implement the sigmoid function and its derivative below.\n",
    "</div>\n",
    "<div style=\"margin:2em 0.2em;margin-top:-2.5em;background-color:#efe;border-radius:0 0 0.5em 0.5em;padding:1.8em;color:#444;\">\n",
    "    <div style=\"color:#5a5;font-weight:bold;float:left;margin-right:1.5em;\">Hint</div>\n",
    "    You will need to use the function [np.exp](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "T7cX9VEangO2"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  \"\"\"\n",
    "  Computes the sigmoid function sigm(input) = 1/(1+exp(-input))\n",
    "  \"\"\"\n",
    "  return 0.0\n",
    "\n",
    "def sigmoid_(y):\n",
    "  \"\"\"\n",
    "  Computes the derivative of sigmoid funtion. sigmoid(y) * (1.0 - sigmoid(y)). \n",
    "  The way we implemented this requires that the input y is already sigmoided\n",
    "  \"\"\"\n",
    "  return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFnCYJiapAfw"
   },
   "source": [
    "And to better understand what these do we'll plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1523979975866,
     "user": {
      "displayName": "Seonwook Park",
      "photoUrl": "//lh6.googleusercontent.com/-BW7qJkFoH5g/AAAAAAAAAAI/AAAAAAAAUbU/YbzlDQf4S14/s50-c-k-no/photo.jpg",
      "userId": "104903917274863991821"
     },
     "user_tz": -120
    },
    "id": "CZwNQ8WDg6dI",
    "outputId": "da67b74b-c297-447a-b4bb-02a62a2f1aa0"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10., 10., num=100)\n",
    "sig = sigmoid(x)\n",
    "sig_prime = sigmoid_(sig)\n",
    "\n",
    "plt.plot(x, sig, label=\"sigmoid\")\n",
    "plt.plot(x, sig_prime, label=\"sigmoid prime\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(prop={'size' : 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_809yZos6Z2"
   },
   "source": [
    "If you need some background info on how we got here check out the full derivation of this [here](http://kawahara.ca/how-to-compute-the-derivative-of-a-sigmoid-function-fully-worked-example/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhLZ_QK_wGiQ"
   },
   "source": [
    "# Backpropagation\n",
    "Before we can build a NN to actually do anything useful, we need a way to train it. The backbone of all things Neural Network training is the backpropagation algorithm. A full treatment goes beyond the scope of this course but when we discussed the MLP we have already seen a simplified version of the backprop algorithm in action.\n",
    "\n",
    "The NN training algorithm consists of 3 subtasks:\n",
    "\n",
    "\n",
    "1.   Make a forward pass (to compute the current, most likely incorrect, prediction)\n",
    "2.   Calculate the error with respect to the ground truth label\n",
    "3.   Make a backward pass to compute an update of the network's parameters\n",
    "\n",
    "The final step propagates the error through the network, starting from the final layer. Thus, the weights (network parameters) get updated based on the error, little by little.\n",
    "\n",
    "If you want to make headway in terms of deep-learning it is important to really understand backprop. Here is a [good summary of the algorithm by Andrew Ng](https://www.youtube.com/watch?v=mOmkv5SI9hU):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1523979976426,
     "user": {
      "displayName": "Seonwook Park",
      "photoUrl": "//lh6.googleusercontent.com/-BW7qJkFoH5g/AAAAAAAAAAI/AAAAAAAAUbU/YbzlDQf4S14/s50-c-k-no/photo.jpg",
      "userId": "104903917274863991821"
     },
     "user_tz": -120
    },
    "id": "B8SrMYkbx4f4",
    "outputId": "da005a00-d48d-4260-fa69-ff6edcb57e56"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/mOmkv5SI9hU\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/mOmkv5SI9hU\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YIGvAO78yXCs"
   },
   "source": [
    "# Solving the XOR problem with a NN\n",
    "Let's now put the pieces we've got so far together to solve a small but non-linear problem first. \n",
    "\n",
    "We will try to create a network to properly predict values from the XOR function. Here is its ground truth table:\n",
    "\n",
    "| Input1 | Input2 | Output |\n",
    "|--------|--------|--------|\n",
    "|   0    |   0    |    0   |    \n",
    "|   0    |   1    |    1   |    \n",
    "|   1    |   0    |    1   |\n",
    "|   1    |   1    |    0   | \n",
    "\n",
    "The result is only \"true\" *iff* either of the inputs is \"one\" but not otherwise.\n",
    "\n",
    "This can be visualized as follows where *green* represents `1` and *red* represents `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1523979976990,
     "user": {
      "displayName": "Seonwook Park",
      "photoUrl": "//lh6.googleusercontent.com/-BW7qJkFoH5g/AAAAAAAAAAI/AAAAAAAAUbU/YbzlDQf4S14/s50-c-k-no/photo.jpg",
      "userId": "104903917274863991821"
     },
     "user_tz": -120
    },
    "id": "DOvDyV-m0ju4",
    "outputId": "2ae0366b-6de8-4310-9c43-004fc642b773"
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([ [0],   [1],   [1],   [0]])\n",
    "\n",
    "# Colors corresponding to class labels y.\n",
    "colors = ['green' if y_ == 1 else 'red' for y_ in y] \n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(6)\n",
    "fig.set_figheight(6)\n",
    "plt.scatter(X[:,0],X[:,1],s=200,c=colors)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cfBth9d16qg"
   },
   "source": [
    "Let's get started by defining our network structure and some NN parameters (often called hyperparameters).\n",
    "In this case you're supposed to define a NN with a single hidden-layer with three units. The NN also has one input layer with two units and an output layer with a single unit. Below is a picture of the network structure. Each circle indicates one unit, each arrow one weight. The units of the hidden layer are followed by an activation function.\n",
    "![A picture of a neural network](data/XOR-MLP.png)\n",
    "\n",
    "<div style=\"margin:2em 0.2em;background-color:#eef;border-radius:0.5em;padding:3.5em;padding-bottom:2.7em;color:#444;\">\n",
    "    <div style=\"color:#55a;font-weight:bold;margin:-1.7em 0 1em -1.7em;\">Task 1b</div>\n",
    "    Set the variables `input_size`, `hidden_size`, and `output_size` as appropriate, by referring to the network specification and diagram above.\n",
    "    <br><br>\n",
    "    Afterwards, set the correct dimensions for the weights `w_hidden` and `w_output`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UR0kDRh82jx0"
   },
   "outputs": [],
   "source": [
    "input_size = None\n",
    "hidden_size = None\n",
    "output_size = None\n",
    "\n",
    "learning_rate = 0.2\n",
    "epochs = 4001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDZp8QmG2quM"
   },
   "source": [
    "Now initialize the weights of our network using random numbers. Make sure you use the proper sizes defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VXOfuK392yas"
   },
   "outputs": [],
   "source": [
    "w_hidden = np.random.uniform(size=(None, None))\n",
    "w_output = np.random.uniform(size=(None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB9pHWlu3A_m"
   },
   "source": [
    "Finally, we'll implement the Backprop algorithm in order to train our network. Remember that this consists of three steps:\n",
    "\n",
    "\n",
    "1.   Forward pass\n",
    "2.   Error calculation\n",
    "3.   Backprop and weight update\n",
    "\n",
    "\n",
    "Linear layers (in this case the output layer) compute their output of the form $\\mathbf{x}^\\mathrm{T}\\mathbf{w}$ and the non-linear (hidden) layer passes this through the activation function $\\sigma(\\mathbf{x}^\\mathrm{T}\\mathbf{w})$. Since the outputs of the lower layers are the inputs of the upper layers you need to start computing results bottom-up.\n",
    "\n",
    "More explicitly, a forward pass (inference) in the above neural network can be represented with two equations. One which outputs hidden layer activations from given single input $\\mathbf{x}$:\n",
    "\n",
    "$$\\mathbf{a} = \\sigma\\left(\\mathbf{h}\\right) = \\sigma\\left( \\mathbf{x}^\\mathrm{T}\\mathbf{W}_1 \\right)$$\n",
    "\n",
    "and one which calculates the final output:\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{a}^\\mathrm{T}\\mathbf{W}_2$$\n",
    "\n",
    "<br>\n",
    "**Note:**\n",
    "In this task, we stack our $4$ data entries (for XOR) into one input matrix $\\mathbf{X}$, row-wise, allowing us to write:\n",
    "\n",
    "$$\\mathbf{h} = \\mathbf{X}\\mathbf{W}_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB9pHWlu3A_m"
   },
   "source": [
    "<div style=\"margin:2em 0.2em;background-color:#eef;border-radius:0.5em;padding:3.5em;padding-bottom:2.7em;color:#444;\">\n",
    "    <div style=\"color:#55a;font-weight:bold;margin:-1.7em 0 1em -1.7em;\">Task 1c</div>\n",
    "    Let's begin by implementing the forward pass in the function named `forward_pass`.\n",
    "</div>\n",
    "<div style=\"margin:2em 0.2em;margin-top:-2.5em;background-color:#efe;border-radius:0 0 0.5em 0.5em;padding:1.8em;color:#444;\">\n",
    "    <div style=\"color:#5a5;font-weight:bold;float:left;margin-right:1.5em;\">Hint</div>\n",
    "    You may need to use [np.ndarray.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.dot.html) for matrix multiplication.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X):\n",
    "    # Step 1: Calculate weighted average of inputs (shape: 4x10)\n",
    "    net_hidden = None\n",
    "    \n",
    "    # Step 2: Calculate the result of the sigmoid activation function (shape: 4x10)\n",
    "    act_hidden = None\n",
    "    \n",
    "    # Step 3: Calculate output of neural network (shape: 4x1)\n",
    "    output = None\n",
    "    \n",
    "    return act_hidden, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB9pHWlu3A_m"
   },
   "source": [
    "To compute the weight update we need to:\n",
    "\n",
    "\n",
    "1.   Compute the derivative of the layer wrt to its input (and pass it on per chain-rule, weighted by the layer's parameters)\n",
    "2.   Compute the derivative of the hidden-layer wrt to it's weights\n",
    "3.   Update the weights\n",
    "\n",
    "For step 1, we must consider how the neural network output is calculated with respect to each weight matrix $\\mathbf{W}_1$ and $\\mathbf{W}_2$. To give you a starting point, we can remind you that the full neural network calculates:\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\sigma\\left( \\mathbf{X}\\mathbf{W}_1 \\right)\\mathbf{W}_2$$\n",
    "\n",
    "A mean-squared error is calculated from this output with,\n",
    "\n",
    "$$\\mathbf{\\epsilon} = \\| \\hat{\\mathbf{y}} - \\mathbf{y} \\|^2_2$$\n",
    "\n",
    "and then the required gradients can be written as:\n",
    "\n",
    "$$\\frac{\\partial\\mathbf{\\epsilon}}{\\partial\\mathbf{W}_2}\n",
    "=\\frac{\\partial\\mathbf{\\epsilon}}{\\partial\\hat{\\mathbf{y}}}\n",
    "\\frac{\\partial\\hat{\\mathbf{y}}}{\\partial\\mathbf{W}_2}$$\n",
    "and\n",
    "$$\\frac{\\partial\\mathbf{\\epsilon}}{\\partial\\mathbf{W}_1}\n",
    "=\\frac{\\partial\\mathbf{\\epsilon}}{\\partial\\hat{\\mathbf{y}}}\n",
    "\\frac{\\partial\\hat{\\mathbf{y}}}{\\partial\\mathbf{a}}\n",
    "\\frac{\\partial\\mathbf{a}}{\\partial\\mathbf{h}}\n",
    "\\frac{\\partial\\mathbf{h}}{\\partial\\mathbf{W}_1}$$\n",
    "\n",
    "\n",
    "<br><br>\n",
    "For step 3, remember that a gradient update for $\\mathbf{W}$ given gradient $\\nabla\\mathbf{\\epsilon}\\left(\\mathbf{W}\\right)=\\frac{\\partial\\mathbf{\\epsilon}}{\\partial\\mathbf{W}}$ is done in the following way:\n",
    "\n",
    "$$\\mathbf{W}^\\prime = \\mathbf{W} - \\lambda\\cdot\\nabla\\mathbf{\\epsilon}\\left(\\mathbf{W}\\right)$$\n",
    "\n",
    "where $\\lambda$ is the learning rate and $\\mathbf{W}^\\prime$ are the updated weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB9pHWlu3A_m"
   },
   "source": [
    "<div style=\"margin:2em 0.2em;background-color:#eef;border-radius:0.5em;padding:3.5em;padding-bottom:2.7em;color:#444;\">\n",
    "    <div style=\"color:#55a;font-weight:bold;margin:-1.7em 0 1em -1.7em;\">Task 1d</div>\n",
    "    Fill in the code to complete a backward-pass, and update the weights `w_output` and `w_hidden`.\n",
    "</div>\n",
    "<div style=\"margin:2em 0.2em;margin-top:-2.5em;background-color:#efe;border-radius:0 0 0.5em 0.5em;padding:1.8em;color:#444;\">\n",
    "    <div style=\"color:#5a5;font-weight:bold;float:left;margin-right:1.5em;\">Hint</div>\n",
    "    Be wary of matrix dimensions. `w_output` is of size (10x1) whereas `w_hidden` is of size (2x10).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2365,
     "status": "ok",
     "timestamp": 1523979980702,
     "user": {
      "displayName": "Seonwook Park",
      "photoUrl": "//lh6.googleusercontent.com/-BW7qJkFoH5g/AAAAAAAAAAI/AAAAAAAAUbU/YbzlDQf4S14/s50-c-k-no/photo.jpg",
      "userId": "104903917274863991821"
     },
     "user_tz": -120
    },
    "id": "U4Fjzjti3Xwq",
    "outputId": "ede2483b-7eb4-46f6-86a8-1fa3ec23a39d"
   },
   "outputs": [],
   "source": [
    "training_errors = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    " \n",
    "    # Step 1: Forward pass\n",
    "    act_hidden, output = forward_pass(X)\n",
    "    \n",
    "    # Step2: Calculate error\n",
    "    residual = output - y\n",
    "    error = 0.5 * (residual ** 2)\n",
    "    \n",
    "    # print error every 200 epochs\n",
    "    if epoch % 200 == 0:\n",
    "        print('Epoch %d> Training error: %f' % (epoch, np.sum(np.abs(error))))\n",
    "\n",
    "    # Step 3: Backward pass\n",
    "    # a) calculate gradient wrt w_output (w2)\n",
    "    de_do = None\n",
    "    do_dw2 = None\n",
    "    de_dw2 = do_dw2.dot(de_do)\n",
    "    \n",
    "    # b) calculate gradient wrt w_hidden (w1)\n",
    "    do_da = None\n",
    "    da_dh =  None\n",
    "    dh_dw1 = None\n",
    "    de_dw1 = dh_dw1.dot(de_do.dot(do_da.T) * da_dh)\n",
    "    \n",
    "    # c) apply gradients with learning rate applied\n",
    "    w_output = None\n",
    "    w_hidden = None\n",
    "    \n",
    "    training_errors.append([epoch, np.mean(error)])\n",
    "\n",
    "# Plot training error progression over time\n",
    "training_errors = np.asarray(training_errors)\n",
    "plt.plot(training_errors[:, 0], training_errors[:, 1]);\n",
    "plt.xlabel('Epochs');\n",
    "plt.ylabel('Training Error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zB5U34iE6Aae"
   },
   "source": [
    "If everything is correct the error should be going down. Play around with epochs and learning rate to get a feeling of what is going on here (make sure to re-run the cell initializing the weights).\n",
    "\n",
    "\n",
    "# Prediction:\n",
    "Now that we have a trained neural network let's try and see if it can actually correctly solve the XOR function. Does the following look like what you would expect to be able to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1523979981224,
     "user": {
      "displayName": "Seonwook Park",
      "photoUrl": "//lh6.googleusercontent.com/-BW7qJkFoH5g/AAAAAAAAAAI/AAAAAAAAUbU/YbzlDQf4S14/s50-c-k-no/photo.jpg",
      "userId": "104903917274863991821"
     },
     "user_tz": -120
    },
    "id": "iDBtbGO26jSm",
    "outputId": "9b9f4556-3a2b-41b3-e2f0-cfab3cb6b5d8"
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_hat = [np.round(forward_pass(x)[1]) for x in X]\n",
    "\n",
    "# Colors corresponding to class predictions y_hat.\n",
    "colors = ['green' if y_ == 1 else 'red' for y_ in y_hat] \n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(6)\n",
    "fig.set_figheight(6)\n",
    "plt.scatter(X[:,0],X[:,1],s=200,c=colors)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6Dm2Y10ygtC"
   },
   "source": [
    "Hopefully your network should now be able to solve XOR succesfully. Here are a couple of questions to think about:\n",
    "\n",
    "\n",
    "*   How do we increase the capacity of the network?\n",
    "*   What happens if we add additional layers to the network?\n",
    "*   What happens if we pick a much smaller/larger learning rate?\n",
    "*   When should we stop training? How many epochs is enough?\n",
    "*   Couldn't we solve this with simple linear regression? Try removing the sigmoid function.\n",
    "\n",
    "\n",
    "We're now ready to apply the very same ideas to a more complex task. This way to part II of the tutorial: [Otmar b \\(Eye Gaze\\).ipynb](./Otmar b %28Eye Gaze%29.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "chi-course-2018-XOR-MLP.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
